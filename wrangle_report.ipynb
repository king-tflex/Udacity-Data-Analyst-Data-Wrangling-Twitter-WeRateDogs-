{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: wragle_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This short report describes the wrangling efforts involved in completing the “WeRateDogs” project as part of\n",
    "Udacity’s Data Analysis Nanodegree. The dataset is the tweet archive of Twitter user @dog_rates, also known as WeRateDogs. WeRateDogs is a Twitter account that rates people's dogs with a humorous comment about the dog. \n",
    "\n",
    "The Data Wrangling process consists of:\n",
    "1. Gathering the data\n",
    "2. Assessing the data\n",
    "3. Cleaning the data\n",
    "4. Storing the data\n",
    "\n",
    "## Gathering\n",
    "\n",
    "The Data used for this Project is obtained from three different sources. Each one testing a different way of obtaining a dataset.\n",
    "\n",
    "The first dataset was downloaded manually as a csv file named \"twitter_archive_enhanced.csv\" and was read into a pandas DataFrame. This file consist of the largest amount of data.\n",
    "\n",
    "The second dataset named \"image_predictions.tsv\" is a file present in each tweet according to a neural network. It is hosted on Udacity's servers and was downloaded programmatically using the Requests library.\n",
    "\n",
    "The third and final dataset stored as \"tweet_json.txt\" was obtained using the tweet IDs in the WeRateDogs Twitter archive to query the Twitter API for each tweet's JSON data using Python's Tweepy library. \n",
    "\n",
    "## Assessing\n",
    "\n",
    "After gathering all three pieces of data, they were assessed visually and programmatically for quality and tidiness issues. The datasets were assessed with the aim of identifying at least eight quality issues and two tidiness issues which would then be cleaned to make the data suitable for analysis. \n",
    "\n",
    "Quality refers to issues related to the content of the data, sometimes called dirty data. The standard criteria of\n",
    "completeness, validity, accuracy, and consistency of the data were used to identify quality issues. While tidiness issues are structural issues, sometimes called messy data.\n",
    "\n",
    "Some python syntax such as \"df.head()\", \"df.info()\", etc. were used to visually assess the dataset and some quality issues such as \"wrong datatypes\" were observed. These quality issues was documented as they were being discovered. Then we employed the use of some functions to further assess the dataset and some tidiness issues were also observed and documented. The assessment was done bearing in mind certain specifications that needs to be met as outlined in the \"Step 2: Assessing data\" section on Udacity classroom page.\n",
    "\n",
    "Per the assessment, eight (8) quality issues and two (2) tidiness issues were documented in the \"Accessing Data\" section in the wrangle_act.ipynb Jupyter Notebook.\n",
    "\n",
    "\n",
    "## Cleaning\n",
    "\n",
    "After assessing the datasets and documenting the quality and tidiness issues to be addressed, we proceeded to cleaning the data. The cleaning followed the standard process of stating the issue, defining the issues, writing the code and testing to ensure the modification was effected. \n",
    "\n",
    "First, we made a copy of the datasets and started addressing the documented issues. Some of the quality issues that were cleaned are; First we corrected datatypes of some variables, then extracted the source of the tweets using a function. While assessing we observed some dog names were unrealistic, so we replaced some with their actual names and changed others to null values. In the text column, we discovered that some ratings were actually in decimal and just integers were extracted to the numerator columns, so we converted the ratings column to float and replaced the entries with the actual decimal value.\n",
    "\n",
    "The first tidiness issue we address was to merge the three datasets into one dataframe using \"tweet_id\" as the reference. Then we save the merged dataframe as \"merged_data\" on which we continued the cleaning process. Also, we dropped certain columns that had a lot of missing values and would not be essential for analysis. \n",
    "\n",
    "After addressing the quality and tidiness issue documented, our new dataframe \"merged_data\" was reduced to 12 columns and 1930 observations which is very clean and suitable for further analysis and visualizations.\n",
    "\n",
    "\n",
    "## Storing\n",
    "The tidy master dataset \"merged_data\", was then stored as a CSV file named \"twitter_archive_master.csv\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
